{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N1AK8ZS9JLAf",
    "outputId": "04d9ff86-8d7e-481e-d8fc-dbf4597deb63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MemTotal:       13290460 kB\n"
     ]
    }
   ],
   "source": [
    "!cat /proc/meminfo | grep MemTotal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLr7HF4lJOzp"
   },
   "outputs": [],
   "source": [
    "a = []\n",
    "while True:\n",
    "    a.append(' ' * 10**6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "9HXpxXRyH6NT",
    "outputId": "6e6660a3-af7b-4562-824b-fc94579a03df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m394.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m892.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m550.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592,
     "referenced_widgets": [
      "335c95fe60f04e1eb526583260b2e7bd",
      "ec8c969e922e4e4f94b869c15ed641f7",
      "0d3c6fb500cb4979bce86eb07dd86af6",
      "8cabefbe43ae4e65a28a4859598cc9a3",
      "094b9a6c65d94274a2cdd0262e30561a",
      "5176ec290d644239b0ca533f352ac301",
      "0b5eb68102d04ce6a3a7c2f7a5f7de6c",
      "278fe397e87d442eb34af0db09c5067e",
      "16b2af9f44114bafb6714f63bb06c4ea",
      "550370c070cd45c4b1e162cbbe69dabe",
      "7f7895cd5e7c447ab95f817f0113a685",
      "a2b502fad6c242d891a43b05690b6d43",
      "b3a7e8c3e73240f882f5fa75dfefb4c5",
      "ec3e19844165483aa162608c1672aed0",
      "2417fc01a21b40ecb0371413d6d64fc6",
      "bbab92516a7f4dc0989ec533ee550510",
      "083d6cdcfb9f4bdb80d55579474a3e52",
      "74eab0bed91741259f7cc72fa3337155",
      "44df9dff560f4d2787274896be65bdf8",
      "5f0084b3e465443cbc55d546635d583c",
      "74e844eb74554d81b5d24eaf5c0ca32d",
      "03cd6bbdb8194a34932a61b0b44d7028",
      "eb985ea907fd4d28930a0f61f1f01226",
      "67cf814ff28c4629912e79dbc3c2dc1c",
      "d69325a656214a3387338269b3c6ae14",
      "d28f696780904bd7adfd0eac5f21da5f",
      "ff8eeff44f0b4f04978c23d1bc23f996",
      "e54225af90844113b92ce0a53ab3a64e",
      "e930786439b242da913f7bbd5e7fe7f4",
      "77554712beac4a6aa227988da4ae7a03",
      "d927db7152a84adc8884e5445486d50d",
      "7c1f35b2d3814e818f17881dadf447bd",
      "c8970c8f8dff4ae6a48e39de314357d1",
      "69547ed82a864677b8e710eeac2a28b2",
      "c8c6b4a742014cd1bab722a6377e702b",
      "48177d3ae703430398f5748855dc671f",
      "614bdf37b08e4d64a222ce9dbdd6d249",
      "7f6ea78f6fe5422ba6533bdc31d4299e",
      "a31cbcacfb76470c88cedf19ce67a756",
      "ef02843406ae44fb83c1136eff6af5de",
      "ec67f813f23944889b3859176ca74c21",
      "fea3bd2d425841c09b94b09ca9333b47",
      "a3fee64d9ccd4f249c3e05858f2e3a5e",
      "72b48f91382b4931981b5cc3dc5b62d4",
      "5bc3e48ca55d42f18157f79ec5a3c993",
      "9d3af78550aa4a40b634b26ed6754c2a",
      "2497d055e4c3453fb24b31dc9c64853d",
      "80cc96d6f74c456ca7ca669df4d20448",
      "ff722e11d1be4514b6b67bb423f1f0d5",
      "09e729bbaa8a4c45b95839e5e75ef869",
      "3d710579853b4f70b03a44f2ea5e9a3f",
      "bef2fcaa50c6449fbe9f325840fc4421",
      "a860efe538e349f5ad7644a11c24a320",
      "0d9aaf5a5bea4a7987db776ce6979571",
      "49cb5f4892544bd4a19fa6e4f5a31a14",
      "67523910686d431abf3cdaaa61c2b652",
      "2c9c2ab9229f4a718cdb6cc5c936bb9a",
      "2256d0edc0234bd4b86f71d14fc8947d",
      "e80c2d975e9d4e66b0e957fba673b423",
      "37380beee733487c87dc6ab50d1c4825",
      "62261c2b3ff040cd90c68cf4d8be0ab0",
      "2741dae3ab5247778b92397be1b9fe3b",
      "93c70c409f7240b38abf2835f01f9f52",
      "83c1657ed4614346868b2ec7d911c3af",
      "53bf0f6a7a91442980eb63fa23a31d28",
      "47fa7b4778344d42b2e9d1ff28824923",
      "f1a832bb171c4441aaab93f9acac14f6",
      "beb2aca886d44f65a119f4b434a29c03",
      "fc2909cc535d47fca0e7ad14f432787d",
      "e1d27cbd2f9443adbef40791fc32cb46",
      "c4cec949d8e7495587c1e78fbe19f97a",
      "10cf445481374e55a01a09cfe41ddf4a",
      "8ab5f61b5c0a4936ac56dc3f201dbda8",
      "21c8014000974be6bcd02e6e9594a1f6",
      "23b75a4ed72f4f548c39712e3dd07b29",
      "6e5c863d803d4e6f971d6b77bc7b97ab",
      "8f42c1ad4109461090fb3618d5eb2791",
      "7b3d0498c7e74b649d8227bf60f9a551",
      "a4f2f6cd8c3f47fc92df5bcaaf3f9aed",
      "79b1cbaf5d6b400798b854d5732c94fb",
      "43cc125d11fe4da4ab726785d9639d8c",
      "88ab66a7e9f64b2bb098df8df43b544c",
      "1bd1c4252323405b825a41d03ab4e542",
      "7c490f9eb0f74ec3be32289e8fb6cd36",
      "5aff2fc79a90463d8a2879b7260e0b2d",
      "ebf0b49aa72f494b8ab071fd54c36f3a",
      "34533cd26c9f444e9c1c7550078c590b",
      "d6a38ecc3d0342f397d739420b58600f",
      "5adec9e218554d1db751cd2ee672f0f0",
      "abd210a26d8349ec924a5564bdcc9db5",
      "4955b8301d79454fbc4addfeea41c53b",
      "845e4c9ef2344823817d6326360ea126",
      "b7554b7c20b84f869fa0a80744916b1e",
      "be5c64ba4d9e4fab9d73ac90896fd2b8",
      "632bfe62e12b42be9e8b68e5c302b8e1",
      "fec4cffdfb294266be8358f9336e71ff",
      "df1428240ed84f009d9574d6ec343b77",
      "da580436506847bbab2a2f8c8230c54f",
      "5d23106248e248cb93e6bd34158eb31c",
      "394ba19ced8c48f0ae4d3e60a6e47f81",
      "855c3a126b464e29aeff1f2681b30d06",
      "b51ed60a7aca49f38542dad2c5b1eaa5",
      "13ee2dc0bd9d4451a486c8efd55c97a5",
      "bc311201e979457caa6b87e7c4933cc1",
      "2df90864cfec4272a77fcfef631b1cbf",
      "02cdc9298a31456f8aaa4b98c127195e",
      "f1e4cd9f32d74ee99dbfbe400eefa81e",
      "895f592c28a04e96b16b5402ded6d5f8",
      "2d1dc40bb7b74a18a6e9c80b0678a478",
      "9c0d5540cc33452b986dbb9c21ccd63c",
      "21a298e7dd37494897d3610b55f49c1d",
      "028e0d107ee243fba02cf6d7e02ba0b2",
      "ba55f0fb5a184ee08f31dded686fde05",
      "e64757596693432db1c6af36cda7e370",
      "212a9885b373448db156cbc1226173ae",
      "dd2dbf912e1a4324977f4411b3b40b5f",
      "f45f8ce682f14ddd8bcefdf0ba6b510a",
      "ad25424900a746e398da36f517a325f3",
      "4246f5afe83e455ea552044eed4e9c26",
      "78ca9171192e488c9955591610e4f528",
      "08c7dd9ad4014807ae3d097bf2a7145d",
      "0d9f3650692942dab7330ad5fe892205",
      "dfe0e691f8824d439968473815a2c9e1",
      "a0fc3819f41e48dab45fa31654b457c9",
      "0abaad18493c4e7c835d802ab5747a6c",
      "00e9b4c523e0495abde61c0f8a292353",
      "8aa01bacee81402b80a4ca3a8e9085df",
      "8a0bb8b2b0a34abfbff4e85dde314497",
      "22f701d220594e4d9162d3c9598f26d3",
      "e5a2169e22474b0191908365dbc7d30b",
      "25af6d9fb7874546872a7bd7f9d05269",
      "5e05ba6b7d014cd2bfe70f7580f6c770",
      "86a5b507a7ec47149a1fb04d55bae9d5",
      "ccba86fcfb0c464bbae39311048865f5",
      "9a34f9a290ef46f8a170c27caa383245",
      "65398596f4f444629ddb3d5261056ff5",
      "01b8ee308fcc44b996cb3a8fd4d472a2",
      "6f8bc7b4749d4697a8c667037640f8e5",
      "ce9eabd959554ab49612084134a06777",
      "2e451bff217d4d39acadddb67251a259",
      "535db4d853e3405faad0ea26e602ea50",
      "104f5e77c9094233bc9690349066b1b0",
      "cf868fe426954ac1a3f2f9b7b7dac2bd"
     ]
    },
    "id": "b3r6h7cmH90u",
    "outputId": "dace30ab-491b-45a8-cb56-a76335df5973"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "335c95fe60f04e1eb526583260b2e7bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2b502fad6c242d891a43b05690b6d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb985ea907fd4d28930a0f61f1f01226",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69547ed82a864677b8e710eeac2a28b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc3e48ca55d42f18157f79ec5a3c993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/434 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67523910686d431abf3cdaaa61c2b652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/712 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a832bb171c4441aaab93f9acac14f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3d0498c7e74b649d8227bf60f9a551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5adec9e218554d1db751cd2ee672f0f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.94G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "394ba19ced8c48f0ae4d3e60a6e47f81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21a298e7dd37494897d3610b55f49c1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/3.59G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d9f3650692942dab7330ad5fe892205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86a5b507a7ec47149a1fb04d55bae9d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/195 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the disk and cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot:  I'm feeling lonely and anxious. What can I do? [/INST] It sounds like you are struggling with loneliness and anxiety, which can be difficult to manage. The first thing that comes to mind is: try talking about it! If you donâ€™t feel comfortable sharing your feelings with friends or family members who live nearby, consider joining an online support group where people share their experiences in a safe environment. Additionally, there might be local events happening near you such as festivals, concerts or art exhibits - maybe something related will interest you and help alleviate some of the stress caused by being alone during holidays (or any other time). Remember: everyone has different preferences when it comes down choosing activities but finding someone else who shares similar interests may make all the difference. Do not hesitate asking professionals for further guidance if needed; they have received extensive training on how best assist clients dealing with these issues. Good luck! ğŸ™‚ ğŸ’• ğŸ‰)â€¦\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model details\n",
    "model_name = \"tanusrich/Mental_Health_Chatbot\"\n",
    "\n",
    "# Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n",
    "\n",
    "# Move model to GPU (if available)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "\n",
    "# Function to generate response\n",
    "def generate_response(user_input):\n",
    "    inputs = tokenizer(user_input, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_k=50,\n",
    "            top_p=0.9,\n",
    "            repetition_penalty=1.2,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "# Example interaction\n",
    "user_input = \"I'm feeling lonely and anxious. What can I do?\"\n",
    "response = generate_response(user_input)\n",
    "print(\"Chatbot: \", response)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
